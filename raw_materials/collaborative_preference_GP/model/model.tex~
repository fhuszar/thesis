\section{Multi-user preference learning \label{sec:model}}

Consider $I$ items with feature vectors $\mathbf{x}_i\in\mathcal{X}$ for $i=1,\ldots,I$. 
The single-user learning approach assumes an independent latent function for the $u$-th user,
$g_u:\mathcal{X}^2\mapsto\mathbb{R}$. Our approach to the multi-user problem is to assume common structure
in the user latent functions. In particular, we assume a set of $D$ shared latent functions,
$h_d:\mathcal{X}^2\mapsto \mathbb{R}$ for $d=1,\ldots,D$, such that the user latent functions are 
generated by a linear combination of these functions, namely

\vspace{-0.5cm}
{\small
\begin{equation}
g_{u}(\mathbf{x}_j,\mathbf{x}_k)=\sum_{d=1}^{D}w_{u,d}h_{d}(\mathbf{x}_j,\mathbf{x}_k)\,,\label{eq:expressionG}
\end{equation}
}

\vspace{-0.5cm}
\normalsize here $w_{u,d}\in \mathbb{R}$ is the weight given to function $h_d$ for user $u$.
We place a GP prior over the shared latent functions $h_{1},\ldots,h_{D}$ using the
preference kernel described in the previous section.
This model allows the preferences of the different users to share
some common structure represented by the latent functions $h_{1},\ldots,h_{D}$.
This approach is similar to dimensionality reduction methods that are commonly used for addressing collaborative filtering problems \cite{stern2009,raiko2007}.

We may extend this model further to the case in which, for each user $u$, there is
a feature vector $\mathbf{u}_u \in \mathcal{U}$ containing information that might be useful for prediction.
We denote by $\mathbf{U}$ the set of all the users' feature vectors,
that is, $\mathbf{U} = \{\mathbf{u}_1,\ldots,\mathbf{u}_U\}$.
The user features are incorporated now by placing a separate GP prior over the users weights.
In particular, we replace the scalars $w_{u,d}$ in (\ref{eq:expressionG}) with functions
$w_d'(\mathbf{u}_u):\mathcal{U}\rightarrow\mathcal{\mathbb{R}}$. 
These weight functions describe the contribution of shared latent function
$h_d$ to the user latent function $g_u$ as a function of the user feature vector $\mathbf{u}_u$.

In the multi-user setting we are given a list
$\List=\{p_1,\ldots,p_P\}$ with all the \emph{pairs} of items evaluated by the users, where $P\leq I(I-1)/2$ (the maximum number of pairs).
The data consists of $\List$, the sets of feature vectors for the users $\mathbf{U}$ (if available),
the item features $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_I\}$, and $U$ sets of preference judgements,
one for each user, $\mathcal{D}=\{\{z_{u,i},y_{u,i}\}_{i=1}^{M_u}\}_{u=1}^{U}$, where $z_{u,i}$ indexes the $i$-th
pair evaluated by user $u$, $y_{i,u}=1$ if this user
prefers the first item in the pair to the second and $y_{i,u}=-1$ otherwise. $M_u$ is the number of 
preference judgements made by the $u$-th user.

\subsection{Probabilistic description}

To address the task of predicting preference on unseen item pairs we cast the model into a probabilistic framework.
%The task of interest is to predict preference on unseen item pairs for a particular user.
%To do this we cast the model described above into a probabilistic framework.
Let $\mathbf{G}$ be an $U\times P$ `user-function' matrix, where each row corresponds to
a particular user's latent function, that is, the entry in the $u$-th column and $i$-th row is 
$g_{u,i}= g_u(\mathbf{x}_{\alpha(i)},\mathbf{x}_{\beta(i)})$
and $\alpha(i)$ and $\beta(i)$ denote respectively the first and second item in the $i$-th pair from $\mathcal{L}$.
Let $\mathbf{H}$ be a $D\times P$ `shared-function' matrix,
where each row represents the shared latent functions, that is, the entry in the $d$-th row and $i$-th column is 
$h_{d,i}= h_d(\mathbf{x}_{\alpha(i)},\mathbf{x}_{\beta(i)})$.
Finally, we introduce the $U \times D$ weight matrix $\mathbf{W}$ such that each row contains a user's weights, that is, the entry in the $u$-th row and $d$-th column of this matrix is $w_d'(\mathbf{u}_u)$.
Note that $\mathbf{G} = \mathbf{W} \mathbf{H}$ represents equation (\ref{eq:expressionG}) in matrix form.
Let $\mathbf{T}$ be the $U\times P$ target matrix given by $\mathbf{T} = \text{sign}[\mathbf{G} + \mathbf{E}]$,
where $\mathbf{E}$ is an $U \times P$ noise matrix whose entries are sampled i.i.d. from a standard Gaussian distribution and
the function ``$\text{sign}$'' retains only the sign of the elements in a matrix. 
The observations $y_{u,i}$ in $\mathcal{D}=\{\{z_{u,i},y_{u,i}\}_{i=1}^{M_u}\}_{u=1}^{U}$ are
mapped to the corresponding entries of $\mathbf{T}$ using $t_{u,z_{u,i}} = y_{u,i}$.
Let $\mathbf{T}^{(\mathcal{D})}$ and $\mathbf{G}^{(\mathcal{D})}$ represent the elements of $\mathbf{T}$ and $\mathbf{G}$
corresponding only to the available observations $y_{u,i}$ in $\mathcal{D}$.
Then, the likelihood for $\mathbf{G}^{(\mathcal{D})}$ given $\mathbf{T}^{(\mathcal{D})}$ and conditional distribution for $\mathbf{G}^{(\mathcal{D})}$ given $\mathbf{H}$ and $\mathbf{W}$ are

\vspace{-0.675cm}
{\small
\begin{align*}
\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})}) 
= \prod_{u=1}^U \prod_{i=1}^{M_u} \Phi[t_{u,z_{u,i}} g_{u,z_{u,i}}]\,\,\,\text{and}\,\,\,
\mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H}) = 
\prod_{u=1}^{U} \prod_{i=1}^{M_u}\delta[g_{u,z_{u,i}}-\mathbf{w}_u\mathbf{h}_{\cdot,z_{u,i}}]\,
\end{align*}
}

\vspace{-0.575cm}
\normalsize respectively, where $\mathbf{w}_u$ is the $u$-th row in $\mathbf{W}$, $\mathbf{h}_{\cdot,i}$ is the $i$-th column in $\mathbf{H}$
and $\delta$ represents a point probability mass at zero.
We now select the priors for $\mathbf{W}$ and $\mathbf{H}$. 
We assume that each function $w_1',\ldots,w_D'$ is sampled \textit{a priori} from a GP
with zero mean and specific covariance function. Let $\mathbf{K}_\text{users}$ be the $U \times U$ 
covariance matrix for entries in each column of matrix $\mathbf{W}$. Then

\vspace{-0.55cm}
{\small
\begin{equation}
\mathcal{P}(\mathbf{W}|\mathbf{U})=  
\prod_{d=1}^D \mathcal{N}(\mathbf{w}_{\cdot,d}|\mathbf{0},\mathbf{K}_\text{users})\,,\label{eq:priorW}
\end{equation}
}

\vspace{-0.45cm}
\normalsize where $\mathbf{w}_{\cdot,d}$ is the $d$-th column in $\mathbf{W}$.
If user features are unavailable, $\mathbf{K}_\text{users}$ becomes the identity matrix.
Finally, we assume that each shared latent function $h_1,\ldots,h_D$ is sampled \textit{a priori} from a GP
with zero mean and covariance function given by a preference kernel. 
Let $\mathbf{K}_\text{items}$ be the $P \times P$ preference covariance 
matrix for the item pairs in $\List$. The prior for $\mathbf{H}$ is then 

\vspace{-0.55cm}
{\small
\begin{equation}
\mathcal{P}(\mathbf{H}|\mathbf{X},\List) = 
\prod_{j=1}^{D}\mathcal{N}(\mathbf{h}_j|\mathbf{0},\mathbf{K}_\text{items})\,,\label{eq:priorH}
\end{equation}
}

\vspace{-0.45cm}
\normalsize where $\mathbf{h}_j$ is the $j$-th row in $\mathbf{H}$. The resulting posterior for $\mathbf{W}$, $\mathbf{H}$ and $\mathbf{G}^{(\mathcal{D})}$ is

\vspace{-0.45cm}
{\small
\begin{equation}
\mathcal{P}(\mathbf{W},\mathbf{H},\mathbf{G}^{(\mathcal{D})}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\List) =
\frac{\mathcal{P}(\mathbf{T}^{(\mathcal{D})}|\mathbf{G}^{(\mathcal{D})})
\mathcal{P}(\mathbf{G}^{(\mathcal{D})}|\mathbf{W},\mathbf{H})\mathcal{P}(\mathbf{W}|\mathbf{U})\mathcal{P}(\mathbf{H}|\mathbf{X},\List)} 
{\mathcal{P}(\mathbf{T}^{(\mathcal{D}}|\mathbf{X},\List)}\,.\label{eq:post}
\end{equation}
}

\vspace{-0.4cm}
\normalsize Given a new item pair $p_{P+1}$, we can compute the predictive distribution for the preference of the $u$-th user ($1 \leq u \leq U$) on this pair by integrating out the parameters $\mathbf{H},\mathbf{W}$ and $\mathbf{G}^{(\mathcal{D})}$ as follows:

\vspace{-0.65cm}
{\small
\begin{align}
\mathcal{P}(t_{u,P+1}|&\mathbf{T}^{(\mathcal{D})},\mathbf{X},\List,p_{P+1}) =
\int \mathcal{P}(t_{u,P+1}|g_{u,P+1}) \mathcal{P}(g_{u,P+1}|\mathbf{w}_u,\mathbf{h}_{\cdot,P+1})\notag\\
 & \quad \mathcal{P}(\mathbf{h}_{\cdot,P+1}|\mathbf{H},\mathbf{X},\List,p_{P+1})
\mathcal{P}(\mathbf{H},\mathbf{W},\mathbf{G}^{(\mathcal{D})}|\mathbf{T}^{(\mathcal{D})},\mathbf{X},\List)
\,d\mathbf{H}\,d\mathbf{W}\,d\mathbf{G}^{(\mathcal{D})}\,,
\label{eq:predictions}
\end{align}
}

\vspace{-0.5cm}
\normalsize where $\mathcal{P}(t_{u,P+1}|g_{u,P+1})=\Phi[t_{u,P+1}g_{u,P+1}]$,
$\mathcal{P}(g_{u,P+1}|\mathbf{w}_u,\mathbf{h}_{\cdot,P+1})=\delta[ g_{u,P+1} - \mathbf{w}_u \mathbf{h}_{\cdot,P+1}]$,

\vspace{-0.5cm}
{\small
\begin{equation}
\mathcal{P}(\mathbf{h}_{\cdot,P+1}|\mathbf{H},\mathbf{X},\List,p_{P+1})
=\prod_{d=1}^D \mathcal{N}(h_{d,P+1}|\mathbf{k}_\star^\text{T} \mathbf{K}^{-1}_\text{items} \mathbf{h}_d, k_\star -
\mathbf{k}_\star^\text{T}  \mathbf{K}^{-1}_\text{items} \mathbf{k}_\star)
\label{eq:predictive}
\end{equation}
}

\vspace{-0.45cm}
\normalsize $k_\star$ is the prior variance of $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and $\mathbf{k}_\star$ is a $P$-dimensional vector that contains the prior covariances between $h_d(\mathbf{x}_{\alpha(P+1)}, \mathbf{x}_{\beta(P+1)})$
and $h_d(\mathbf{x}_{\alpha(1)}, \mathbf{x}_{\beta(1)}),\ldots,h_d(\mathbf{x}_{\alpha(P)}, \mathbf{x}_{\beta(P)})$.
Computing (\ref{eq:post}) or (\ref{eq:predictive})
is infeasible and approximations must be used.
For this, we use a combination of expectation propagation (EP) \cite{Minka2001} and variation Bayes (VB) \cite{Ghahramani2001}.
Empirical studies show that EP obtains state-of-the-art performance 
in the related problem of GP binary classification \cite{nickisch2008}.

We want to learn user preferences with the proposed model
from the least amount of data possible. Therefore we desire to query
users actively about their preferences on the most informative pairs of items \cite{brochu2007active}.
Next, we describe a novel method to implement this strategy.
This method exploits the preference kernel and so may
be trivially generalized to GP binary classification problems also.
